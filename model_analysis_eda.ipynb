{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc658dad",
   "metadata": {},
   "source": [
    "# üìä Model Performance Analysis & EDA\n",
    "## Interactive Exploration of Model Test Outputs\n",
    "\n",
    "This notebook provides comprehensive analysis of model predictions and performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084fc984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üì¶ Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6920dc5",
   "metadata": {},
   "source": [
    "## 1. Load Available Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52439cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_available_models(predictions_dir=\"data/predictions\"):\n",
    "    \"\"\"Get list of available models\"\"\"\n",
    "    predictions_dir = Path(predictions_dir)\n",
    "    \n",
    "    if not predictions_dir.exists():\n",
    "        return []\n",
    "    \n",
    "    models = []\n",
    "    for model_dir in predictions_dir.iterdir():\n",
    "        if model_dir.is_dir():\n",
    "            # Check if there are prediction files\n",
    "            pred_files = list(model_dir.glob(\"*_test_preds_*.csv\"))\n",
    "            if pred_files:\n",
    "                models.append(model_dir.name)\n",
    "    \n",
    "    return sorted(models)\n",
    "\n",
    "# Get available models\n",
    "available_models = get_available_models()\n",
    "print(f\"üìã Available Models ({len(available_models)}):\")\n",
    "for i, model in enumerate(available_models, 1):\n",
    "    print(f\"  {i}. {model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c083b0c4",
   "metadata": {},
   "source": [
    "## 2. Select and Load Model Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1995b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select model to analyze\n",
    "MODEL_NAME = \"mlp\"  # Change this to analyze different models\n",
    "\n",
    "def load_model_predictions(model_name, predictions_dir=\"data/predictions\"):\n",
    "    \"\"\"Load model predictions and return test and train dataframes\"\"\"\n",
    "    model_dir = Path(predictions_dir) / model_name\n",
    "    \n",
    "    # Get latest files\n",
    "    test_pred_files = list(model_dir.glob(\"*_test_preds_*.csv\"))\n",
    "    train_pred_files = list(model_dir.glob(\"*_train_preds_*.csv\"))\n",
    "    \n",
    "    if not test_pred_files:\n",
    "        print(f\"‚ùå No test prediction files found for {model_name}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Use latest files\n",
    "    latest_test = max(test_pred_files, key=lambda x: x.stat().st_mtime)\n",
    "    latest_train = max(train_pred_files, key=lambda x: x.stat().st_mtime) if train_pred_files else None\n",
    "    \n",
    "    print(f\"üìÇ Loading data for {model_name}:\")\n",
    "    print(f\"  Test predictions: {latest_test.name}\")\n",
    "    if latest_train:\n",
    "        print(f\"  Train predictions: {latest_train.name}\")\n",
    "    \n",
    "    # Load data\n",
    "    test_df = pd.read_csv(latest_test)\n",
    "    train_df = pd.read_csv(latest_train) if latest_train else None\n",
    "    \n",
    "    # Convert timestamps\n",
    "    if 'timestamp' in test_df.columns:\n",
    "        test_df['timestamp'] = pd.to_datetime(test_df['timestamp'])\n",
    "    if train_df is not None and 'timestamp' in train_df.columns:\n",
    "        train_df['timestamp'] = pd.to_datetime(train_df['timestamp'])\n",
    "    \n",
    "    return test_df, train_df\n",
    "\n",
    "# Load the selected model\n",
    "test_df, train_df = load_model_predictions(MODEL_NAME)\n",
    "\n",
    "if test_df is not None:\n",
    "    print(f\"\\n‚úÖ Data loaded successfully!\")\n",
    "    print(f\"üìä Test set shape: {test_df.shape}\")\n",
    "    if train_df is not None:\n",
    "        print(f\"üìä Train set shape: {train_df.shape}\")\n",
    "else:\n",
    "    print(f\"‚ùå Failed to load data for {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b674c9a7",
   "metadata": {},
   "source": [
    "## 3. Data Overview & Summary Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556083c8",
   "metadata": {},
   "source": [
    "## 2.1. System Architecture & Model Information\n",
    "\n",
    "This section provides information about the current system architecture, champion model status, data encoding, and model-specific details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31919520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "def get_champion_model_info():\n",
    "    \"\"\"Get current champion model information\"\"\"\n",
    "    champion_file = Path(\"models/champion.txt\")\n",
    "    if champion_file.exists():\n",
    "        with open(champion_file, 'r') as f:\n",
    "            champion_model = f.read().strip()\n",
    "        return champion_model\n",
    "    return \"Unknown\"\n",
    "\n",
    "def load_model_metadata(model_name, predictions_dir=\"data/predictions\"):\n",
    "    \"\"\"Load model metadata from YAML file\"\"\"\n",
    "    model_dir = Path(predictions_dir) / model_name\n",
    "    metadata_files = list(model_dir.glob(\"*_metadata_*.yaml\"))\n",
    "    \n",
    "    if not metadata_files:\n",
    "        return None\n",
    "    \n",
    "    latest_metadata = max(metadata_files, key=lambda x: x.stat().st_mtime)\n",
    "    \n",
    "    try:\n",
    "        with open(latest_metadata, 'r') as f:\n",
    "            content = f.read()\n",
    "            # Filter out problematic NumPy serializations\n",
    "            lines = content.split('\\n')\n",
    "            filtered_lines = []\n",
    "            skip_lines = False\n",
    "            \n",
    "            for line in lines:\n",
    "                if 'test_metrics:' in line or 'train_metrics:' in line:\n",
    "                    skip_lines = True\n",
    "                    continue\n",
    "                if skip_lines and (line.startswith('  ') or line.strip() == ''):\n",
    "                    continue\n",
    "                if skip_lines and not line.startswith(' '):\n",
    "                    skip_lines = False\n",
    "                \n",
    "                if not skip_lines:\n",
    "                    filtered_lines.append(line)\n",
    "            \n",
    "            filtered_content = '\\n'.join(filtered_lines)\n",
    "            metadata = yaml.safe_load(filtered_content)\n",
    "            \n",
    "            # Ensure we have the model name\n",
    "            if metadata and 'model' not in metadata:\n",
    "                metadata['model'] = model_name\n",
    "        return metadata\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not load metadata: {e}\")\n",
    "        return {'model': model_name}\n",
    "\n",
    "def get_system_architecture_info(model_name, metadata):\n",
    "    \"\"\"Get system architecture and data encoding information\"\"\"\n",
    "    info = {}\n",
    "    \n",
    "    # Champion model status\n",
    "    champion_model = get_champion_model_info()\n",
    "    info['champion_model'] = champion_model\n",
    "    info['is_champion'] = model_name == champion_model\n",
    "    \n",
    "    # Data encoding status\n",
    "    if metadata and 'data_type' in metadata:\n",
    "        data_type = metadata['data_type']\n",
    "        info['data_type'] = data_type\n",
    "        if data_type == 'dense':\n",
    "            info['encoding_status'] = \"‚úÖ Using autoencoder embeddings (64-dimensional dense representation)\"\n",
    "        else:\n",
    "            info['encoding_status'] = \"üìä Using scaled wide format (direct feature encoding)\"\n",
    "    else:\n",
    "        info['encoding_status'] = \"‚ùì Data encoding status unknown\"\n",
    "    \n",
    "    # MLP-specific architecture information\n",
    "    if model_name == 'mlp' and metadata and 'best_params' in metadata:\n",
    "        best_params = metadata['best_params']\n",
    "        info['mlp_architecture'] = {\n",
    "            'hidden_layers': best_params.get('hidden_layer_sizes', 'Unknown'),\n",
    "            'alpha': best_params.get('alpha', 'Unknown'),\n",
    "            'learning_rate_init': best_params.get('learning_rate_init', 'Unknown'),\n",
    "            'max_iter': best_params.get('max_iter', 'Unknown')\n",
    "        }\n",
    "    \n",
    "    return info\n",
    "\n",
    "def display_system_information(model_name, metadata):\n",
    "    \"\"\"Display system architecture and model information\"\"\"\n",
    "    print(f\"{'='*60}\")\n",
    "    print(\"üèóÔ∏è  SYSTEM ARCHITECTURE & MODEL INFORMATION\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Get system info\n",
    "    sys_info = get_system_architecture_info(model_name, metadata)\n",
    "    \n",
    "    # Champion model status\n",
    "    champion_status = \"üèÜ CHAMPION MODEL\" if sys_info.get('is_champion') else \"üìà Non-champion model\"\n",
    "    print(f\"\\nModel Status: {champion_status}\")\n",
    "    print(f\"Current Champion: {sys_info.get('champion_model', 'Unknown')}\")\n",
    "    \n",
    "    # Data encoding information\n",
    "    print(f\"\\nData Architecture:\")\n",
    "    print(f\"  {sys_info.get('encoding_status', 'Unknown')}\")\n",
    "    \n",
    "    if 'data_type' in sys_info:\n",
    "        if sys_info['data_type'] == 'dense':\n",
    "            print(f\"  ‚Ä¢ Features preprocessed through autoencoder neural network\")\n",
    "            print(f\"  ‚Ä¢ 64-dimensional compressed representation\")\n",
    "            print(f\"  ‚Ä¢ Better for capturing complex feature interactions\")\n",
    "        else:\n",
    "            print(f\"  ‚Ä¢ Direct feature scaling and engineering\")\n",
    "            print(f\"  ‚Ä¢ Wide format with original feature structure\")\n",
    "            print(f\"  ‚Ä¢ Better for interpretability and linear relationships\")\n",
    "    \n",
    "    # MLP-specific information\n",
    "    if model_name == 'mlp' and 'mlp_architecture' in sys_info:\n",
    "        arch = sys_info['mlp_architecture']\n",
    "        print(f\"\\nüß† MLP Neural Network Architecture:\")\n",
    "        print(f\"  ‚Ä¢ Hidden Layers: {arch['hidden_layers']} neurons\")\n",
    "        print(f\"  ‚Ä¢ Regularization (alpha): {arch['alpha']:.6f}\")\n",
    "        print(f\"  ‚Ä¢ Learning Rate: {arch['learning_rate_init']:.6f}\")\n",
    "        print(f\"  ‚Ä¢ Max Iterations: {arch['max_iter']}\")\n",
    "        print(f\"  ‚Ä¢ Architecture: Input ‚Üí {arch['hidden_layers'][0]} ‚Üí {arch['hidden_layers'][1]} ‚Üí Output\")\n",
    "\n",
    "# Load metadata and display system information\n",
    "if test_df is not None:\n",
    "    metadata = load_model_metadata(MODEL_NAME)\n",
    "    display_system_information(MODEL_NAME, metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1745f99",
   "metadata": {},
   "source": [
    "## 2.5. Time Range Selection (Optional)\n",
    "\n",
    "Select a specific time range for detailed analysis. This is useful for focusing on specific periods of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db69f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_range_options(test_df):\n",
    "    \"\"\"Get available time range options\"\"\"\n",
    "    if 'timestamp' not in test_df.columns:\n",
    "        return {}\n",
    "    \n",
    "    test_df['timestamp'] = pd.to_datetime(test_df['timestamp'])\n",
    "    start_date = test_df['timestamp'].min()\n",
    "    end_date = test_df['timestamp'].max()\n",
    "    full_duration = end_date - start_date\n",
    "    \n",
    "    options = {}\n",
    "    \n",
    "    # Always include full range\n",
    "    options['full'] = ('Full Range', start_date, end_date)\n",
    "    \n",
    "    # Add recent periods\n",
    "    if full_duration.days >= 30:\n",
    "        options['last_30'] = ('Last 30 Days', \n",
    "                            max(start_date, end_date - pd.Timedelta(days=30)), end_date)\n",
    "    if full_duration.days >= 14:\n",
    "        options['last_14'] = ('Last 14 Days', \n",
    "                            max(start_date, end_date - pd.Timedelta(days=14)), end_date)\n",
    "    if full_duration.days >= 7:\n",
    "        options['last_7'] = ('Last 7 Days', \n",
    "                           max(start_date, end_date - pd.Timedelta(days=7)), end_date)\n",
    "    \n",
    "    # Add quarterly options if data spans multiple years\n",
    "    if full_duration.days > 365:\n",
    "        current_year = end_date.year\n",
    "        options['q4'] = (f'Q4 {current_year}', \n",
    "                       max(start_date, pd.Timestamp(f'{current_year}-10-01')), end_date)\n",
    "        options['q3'] = (f'Q3 {current_year}', \n",
    "                       max(start_date, pd.Timestamp(f'{current_year}-07-01')), \n",
    "                       min(end_date, pd.Timestamp(f'{current_year}-09-30')))\n",
    "    \n",
    "    # Filter out options that would be too small\n",
    "    valid_options = {}\n",
    "    for key, (label, opt_start, opt_end) in options.items():\n",
    "        if (opt_end - opt_start).days >= 7:  # At least 7 days\n",
    "            valid_options[key] = (label, opt_start, opt_end)\n",
    "    \n",
    "    return valid_options\n",
    "\n",
    "def select_time_range_interactive(test_df):\n",
    "    \"\"\"Interactive time range selection for Jupyter\"\"\"\n",
    "    options = get_time_range_options(test_df)\n",
    "    \n",
    "    if not options:\n",
    "        print(\"‚ö†Ô∏è No timestamp data available for time range selection\")\n",
    "        return None\n",
    "    \n",
    "    print(\"üìÖ Available Time Ranges:\")\n",
    "    for i, (key, (label, start, end)) in enumerate(options.items(), 1):\n",
    "        duration = end - start\n",
    "        print(f\"  {i}. {label}: {start.date()} to {end.date()} ({duration.days} days)\")\n",
    "    \n",
    "    print(\"\\nüí° You can modify the SELECTED_TIME_RANGE variable below to choose a specific range\")\n",
    "    return options\n",
    "\n",
    "# Set up time range selection\n",
    "if test_df is not None and 'timestamp' in test_df.columns:\n",
    "    time_options = select_time_range_interactive(test_df)\n",
    "    \n",
    "    # You can change this to select different time ranges:\n",
    "    # Options: 'full', 'last_30', 'last_14', 'last_7', 'q4', 'q3'\n",
    "    SELECTED_TIME_RANGE = 'full'  # Change this to focus on specific periods\n",
    "    \n",
    "    if SELECTED_TIME_RANGE in time_options:\n",
    "        selected_range = time_options[SELECTED_TIME_RANGE]\n",
    "        TIME_RANGE_LABEL, TIME_START, TIME_END = selected_range\n",
    "        \n",
    "        # Filter data for visualization (keep original for metrics)\n",
    "        viz_test_df = test_df[(test_df['timestamp'] >= TIME_START) & (test_df['timestamp'] <= TIME_END)].copy()\n",
    "        print(f\"\\n‚úÖ Selected: {TIME_RANGE_LABEL}\")\n",
    "        print(f\"üìä Filtered data: {len(viz_test_df)} samples ({len(viz_test_df)/len(test_df)*100:.1f}% of total)\")\n",
    "    else:\n",
    "        viz_test_df = test_df.copy()\n",
    "        TIME_RANGE_LABEL = \"Full Range\"\n",
    "        print(f\"\\n‚úÖ Using full range for analysis\")\n",
    "else:\n",
    "    viz_test_df = test_df.copy() if test_df is not None else None\n",
    "    TIME_RANGE_LABEL = \"Full Range\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9e68e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_df is not None:\n",
    "    print(f\"üîç DATA OVERVIEW FOR {MODEL_NAME.upper()}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Basic info\n",
    "    print(f\"\\nüìã Columns: {list(test_df.columns)}\")\n",
    "    print(f\"üìè Test set rows: {len(test_df):,}\")\n",
    "    \n",
    "    if 'timestamp' in test_df.columns:\n",
    "        print(f\"üìÖ Date range: {test_df['timestamp'].min()} to {test_df['timestamp'].max()}\")\n",
    "        print(f\"‚è±Ô∏è  Duration: {test_df['timestamp'].max() - test_df['timestamp'].min()}\")\n",
    "    \n",
    "    # Display first few rows\n",
    "    print(f\"\\nüìë First 5 rows:\")\n",
    "    display(test_df.head())\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\nüìä Summary Statistics:\")\n",
    "    display(test_df[['true_count', 'pred_count']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9410f440",
   "metadata": {},
   "source": [
    "## 4. Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926673e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_comprehensive_metrics(df, set_name=\"Test\"):\n",
    "    \"\"\"Calculate comprehensive performance metrics\"\"\"\n",
    "    if df is None or 'true_count' not in df.columns or 'pred_count' not in df.columns:\n",
    "        return {}\n",
    "    \n",
    "    y_true = df['true_count']\n",
    "    y_pred = df['pred_count']\n",
    "    \n",
    "    # Basic metrics\n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "    rmse = np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "    \n",
    "    # Percentage errors\n",
    "    non_zero_mask = y_true != 0\n",
    "    if non_zero_mask.sum() > 0:\n",
    "        mape = np.mean(np.abs((y_true[non_zero_mask] - y_pred[non_zero_mask]) / y_true[non_zero_mask])) * 100\n",
    "    else:\n",
    "        mape = np.inf\n",
    "    \n",
    "    # R-squared\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    r2 = 1 - (ss_res / ss_tot) if ss_tot != 0 else 0\n",
    "    \n",
    "    # Additional metrics\n",
    "    median_ae = np.median(np.abs(y_true - y_pred))\n",
    "    max_error = np.max(np.abs(y_true - y_pred))\n",
    "    \n",
    "    metrics = {\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'MAPE (%)': mape,\n",
    "        'R¬≤': r2,\n",
    "        'Median AE': median_ae,\n",
    "        'Max Error': max_error\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "if test_df is not None:\n",
    "    # Calculate metrics for test set\n",
    "    test_metrics = calculate_comprehensive_metrics(test_df, \"Test\")\n",
    "    \n",
    "    print(f\"üìà PERFORMANCE METRICS FOR {MODEL_NAME.upper()}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(f\"\\nüéØ Test Set Performance:\")\n",
    "    for metric, value in test_metrics.items():\n",
    "        print(f\"  {metric:>12}: {value:10.4f}\")\n",
    "    \n",
    "    # Calculate metrics for train set if available\n",
    "    if train_df is not None:\n",
    "        train_metrics = calculate_comprehensive_metrics(train_df, \"Train\")\n",
    "        print(f\"\\nüèãÔ∏è Train Set Performance:\")\n",
    "        for metric, value in train_metrics.items():\n",
    "            print(f\"  {metric:>12}: {value:10.4f}\")\n",
    "        \n",
    "        # Compare train vs test\n",
    "        print(f\"\\nüìä Train vs Test Comparison:\")\n",
    "        print(f\"  {'Metric':>12} | {'Train':>10} | {'Test':>10} | {'Difference':>12}\")\n",
    "        print(\"-\" * 50)\n",
    "        for metric in ['MAE', 'RMSE', 'R¬≤']:\n",
    "            if metric in train_metrics and metric in test_metrics:\n",
    "                diff = test_metrics[metric] - train_metrics[metric]\n",
    "                print(f\"  {metric:>12} | {train_metrics[metric]:>10.4f} | {test_metrics[metric]:>10.4f} | {diff:>+12.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bf3b28",
   "metadata": {},
   "source": [
    "## 5. Time Series Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de123ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_df is not None:\n",
    "    # Use viz_test_df (filtered data) for visualizations, but keep test_df for metrics\n",
    "    if viz_test_df is not None:\n",
    "        # Create comprehensive time series plots\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "        fig.suptitle(f'Time Series Analysis: {MODEL_NAME.upper()} ({TIME_RANGE_LABEL})', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Calculate key metrics for analysis text\n",
    "        y_true = viz_test_df['true_count']\n",
    "        y_pred = viz_test_df['pred_count']\n",
    "        residuals = y_true - y_pred\n",
    "        mae = np.mean(np.abs(residuals))\n",
    "        rmse = np.sqrt(np.mean(residuals ** 2))\n",
    "        correlation = np.corrcoef(y_true, y_pred)[0, 1]\n",
    "        \n",
    "        # 1. Full time series\n",
    "        ax1 = axes[0, 0]\n",
    "        if 'timestamp' in viz_test_df.columns:\n",
    "            x_data = viz_test_df['timestamp']\n",
    "            xlabel = 'Date'\n",
    "        else:\n",
    "            x_data = range(len(viz_test_df))\n",
    "            xlabel = 'Index'\n",
    "        \n",
    "        ax1.plot(x_data, viz_test_df['true_count'], label='Actual', color='blue', linewidth=1.2, alpha=0.8)\n",
    "        ax1.plot(x_data, viz_test_df['pred_count'], label='Predicted', color='red', linewidth=1.2, alpha=0.8, linestyle='--')\n",
    "        ax1.set_title('Time Series: Predictions vs Actual', fontsize=12, fontweight='bold')\n",
    "        ax1.set_xlabel(xlabel)\n",
    "        ax1.set_ylabel('Count')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add analysis text\n",
    "        time_text = f\"\"\"Analysis: The model shows {'strong' if correlation > 0.8 else 'moderate' if correlation > 0.6 else 'weak'} temporal correlation (r={correlation:.3f}).\n",
    "        MAE={mae:.1f}, RMSE={rmse:.1f}. The predictions {'closely follow' if mae < y_true.mean() * 0.1 else 'generally track' if mae < y_true.mean() * 0.2 else 'loosely follow'} the actual patterns.\n",
    "        {'Seasonal patterns are well captured.' if correlation > 0.7 else 'Some temporal patterns may be missed.'}\"\"\"\n",
    "        ax1.text(0.02, 0.98, time_text, transform=ax1.transAxes, verticalalignment='top', \n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\", alpha=0.7), fontsize=8)\n",
    "        \n",
    "        # 2. Predicted vs Actual Scatter\n",
    "        ax2 = axes[0, 1]\n",
    "        ax2.scatter(viz_test_df['true_count'], viz_test_df['pred_count'], alpha=0.6, color='blue', s=15)\n",
    "        \n",
    "        # Perfect prediction line\n",
    "        min_val = min(viz_test_df['true_count'].min(), viz_test_df['pred_count'].min())\n",
    "        max_val = max(viz_test_df['true_count'].max(), viz_test_df['pred_count'].max())\n",
    "        ax2.plot([min_val, max_val], [min_val, max_val], 'r--', label='Perfect Prediction', linewidth=2)\n",
    "        \n",
    "        ax2.set_title('Predicted vs Actual Values', fontsize=12, fontweight='bold')\n",
    "        ax2.set_xlabel('Actual Count')\n",
    "        ax2.set_ylabel('Predicted Count')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add analysis text\n",
    "        scatter_text = f\"\"\"Analysis: Points {'cluster tightly' if rmse < y_true.std() * 0.5 else 'spread moderately' if rmse < y_true.std() else 'spread widely'} around perfect line.\n",
    "        R¬≤={correlation**2:.3f} indicates {correlation**2*100:.1f}% variance explained.\n",
    "        {'Excellent prediction accuracy' if correlation > 0.9 else 'Good prediction accuracy' if correlation > 0.8 else 'Moderate prediction accuracy' if correlation > 0.6 else 'Room for improvement'} for this range.\"\"\"\n",
    "        ax2.text(0.02, 0.98, scatter_text, transform=ax2.transAxes, verticalalignment='top',\n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgreen\", alpha=0.7), fontsize=8)\n",
    "        \n",
    "        # 3. Residuals distribution\n",
    "        ax3 = axes[1, 0]\n",
    "        ax3.hist(residuals, bins=50, alpha=0.7, color='skyblue', edgecolor='black', density=True)\n",
    "        ax3.axvline(0, color='red', linestyle='--', label='Zero Error', linewidth=2)\n",
    "        ax3.axvline(residuals.mean(), color='orange', linestyle='-', label=f'Mean Error ({residuals.mean():.1f})', linewidth=2)\n",
    "        ax3.set_title('Residuals Distribution', fontsize=12, fontweight='bold')\n",
    "        ax3.set_xlabel('Residual (Actual - Predicted)')\n",
    "        ax3.set_ylabel('Density')\n",
    "        ax3.legend()\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add analysis text\n",
    "        skewness = residuals.skew()\n",
    "        bias_text = f\"\"\"Analysis: Mean error = {residuals.mean():.2f} ({'systematic bias' if abs(residuals.mean()) > rmse * 0.1 else 'minimal bias'}).\n",
    "        Distribution is {'highly skewed' if abs(skewness) > 1 else 'moderately skewed' if abs(skewness) > 0.5 else 'approximately normal'} (skew={skewness:.2f}).\n",
    "        {'Model tends to over-predict' if residuals.mean() < -5 else 'Model tends to under-predict' if residuals.mean() > 5 else 'Model is well-calibrated'}.\"\"\"\n",
    "        ax3.text(0.02, 0.98, bias_text, transform=ax3.transAxes, verticalalignment='top',\n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightyellow\", alpha=0.7), fontsize=8)\n",
    "        \n",
    "        # 4. Absolute error over time\n",
    "        ax4 = axes[1, 1]\n",
    "        abs_errors = np.abs(residuals)\n",
    "        ax4.plot(x_data, abs_errors, color='orange', alpha=0.7, linewidth=1.2)\n",
    "        ax4.axhline(mae, color='red', linestyle='--', label=f'Mean Absolute Error ({mae:.1f})', linewidth=2)\n",
    "        ax4.set_title('Absolute Error Over Time', fontsize=12, fontweight='bold')\n",
    "        ax4.set_xlabel(xlabel)\n",
    "        ax4.set_ylabel('Absolute Error')\n",
    "        ax4.legend()\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add analysis text\n",
    "        error_stability = abs_errors.std()\n",
    "        error_text = f\"\"\"Analysis: Error consistency - std={error_stability:.2f} ({'very stable' if error_stability < mae * 0.5 else 'stable' if error_stability < mae else 'variable'}).\n",
    "        Max error = {abs_errors.max():.1f}, occurs {'frequently' if (abs_errors > mae * 2).mean() > 0.1 else 'occasionally'}.\n",
    "        {'Error patterns suggest model limitations' if abs_errors.max() > mae * 5 else 'Error patterns are reasonable'}.\"\"\"\n",
    "        ax4.text(0.02, 0.98, error_text, transform=ax4.transAxes, verticalalignment='top',\n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightcoral\", alpha=0.7), fontsize=8)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Save the plot\n",
    "        time_suffix = f\"_{TIME_START.strftime('%Y%m%d')}_{TIME_END.strftime('%Y%m%d')}\" if 'TIME_START' in globals() and TIME_RANGE_LABEL != \"Full Range\" else \"\"\n",
    "        output_file = f\"{MODEL_NAME}_enhanced_analysis{time_suffix}.png\"\n",
    "        plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "        print(f\"\\nüìä Enhanced visualization saved as: {output_file}\")\n",
    "        \n",
    "        print(f\"üìä Enhanced time series analysis completed for {MODEL_NAME} ({TIME_RANGE_LABEL})\")\n",
    "    else:\n",
    "        print(\"‚ùå No data available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8828c5cf",
   "metadata": {},
   "source": [
    "## 6. Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac23ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if viz_test_df is not None:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle(f'Distribution Analysis: {MODEL_NAME.upper()} ({TIME_RANGE_LABEL})', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Calculate residuals for filtered data\n",
    "    residuals = viz_test_df['true_count'] - viz_test_df['pred_count']\n",
    "    \n",
    "    # 1. Actual vs Predicted distributions\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.hist(viz_test_df['true_count'], bins=50, alpha=0.7, label='Actual', color='blue', density=True)\n",
    "    ax1.hist(viz_test_df['pred_count'], bins=50, alpha=0.7, label='Predicted', color='red', density=True)\n",
    "    ax1.set_title('Distribution: Actual vs Predicted')\n",
    "    ax1.set_xlabel('Count')\n",
    "    ax1.set_ylabel('Density')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Residuals distribution\n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.hist(residuals, bins=50, alpha=0.7, color='orange', edgecolor='black')\n",
    "    ax2.axvline(0, color='red', linestyle='--', linewidth=2)\n",
    "    ax2.axvline(np.mean(residuals), color='green', linestyle=':', linewidth=2, label=f'Mean: {np.mean(residuals):.2f}')\n",
    "    ax2.set_title('Residuals Distribution')\n",
    "    ax2.set_xlabel('Residual (Actual - Predicted)')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Q-Q plot for residuals\n",
    "    ax3 = axes[1, 0]\n",
    "    from scipy import stats\n",
    "    stats.probplot(residuals, dist=\"norm\", plot=ax3)\n",
    "    ax3.set_title('Q-Q Plot: Residuals vs Normal Distribution')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Absolute error distribution\n",
    "    ax4 = axes[1, 1]\n",
    "    abs_errors = np.abs(residuals)\n",
    "    mae_filtered = np.mean(abs_errors)\n",
    "    median_ae_filtered = np.median(abs_errors)\n",
    "    ax4.hist(abs_errors, bins=50, alpha=0.7, color='purple', edgecolor='black')\n",
    "    ax4.axvline(mae_filtered, color='red', linestyle='--', linewidth=2, label=f'MAE: {mae_filtered:.2f}')\n",
    "    ax4.axvline(median_ae_filtered, color='green', linestyle=':', linewidth=2, label=f'Median AE: {median_ae_filtered:.2f}')\n",
    "    ax4.set_title('Absolute Error Distribution')\n",
    "    ax4.set_xlabel('Absolute Error')\n",
    "    ax4.set_ylabel('Frequency')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"üìä Distribution analysis completed for {MODEL_NAME} ({TIME_RANGE_LABEL})\")\n",
    "else:\n",
    "    print(\"‚ùå No data available for distribution analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b7ed52",
   "metadata": {},
   "source": [
    "## 7. Scatter Plot Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdebfae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_df is not None:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    fig.suptitle(f'Scatter Plot Analysis: {MODEL_NAME.upper()}', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Predicted vs Actual\n",
    "    ax1 = axes[0]\n",
    "    ax1.scatter(test_df['true_count'], test_df['pred_count'], alpha=0.6, color='blue', s=20)\n",
    "    \n",
    "    # Perfect prediction line\n",
    "    min_val = min(test_df['true_count'].min(), test_df['pred_count'].min())\n",
    "    max_val = max(test_df['true_count'].max(), test_df['pred_count'].max())\n",
    "    ax1.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "    \n",
    "    # Add trend line\n",
    "    z = np.polyfit(test_df['true_count'], test_df['pred_count'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    ax1.plot(test_df['true_count'], p(test_df['true_count']), 'g-', alpha=0.8, linewidth=2, label=f'Trend: y={z[0]:.3f}x+{z[1]:.1f}')\n",
    "    \n",
    "    ax1.set_title('Predicted vs Actual')\n",
    "    ax1.set_xlabel('Actual Count')\n",
    "    ax1.set_ylabel('Predicted Count')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add R¬≤ annotation\n",
    "    ax1.text(0.05, 0.95, f'R¬≤ = {test_metrics[\"R¬≤\"]:.4f}', transform=ax1.transAxes, \n",
    "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8), fontsize=12)\n",
    "    \n",
    "    # 2. Residuals vs Predicted\n",
    "    ax2 = axes[1]\n",
    "    residuals = test_df['true_count'] - test_df['pred_count']\n",
    "    ax2.scatter(test_df['pred_count'], residuals, alpha=0.6, color='orange', s=20)\n",
    "    ax2.axhline(0, color='red', linestyle='--', linewidth=2)\n",
    "    \n",
    "    # Add trend line to check for bias\n",
    "    z_res = np.polyfit(test_df['pred_count'], residuals, 1)\n",
    "    p_res = np.poly1d(z_res)\n",
    "    ax2.plot(test_df['pred_count'], p_res(test_df['pred_count']), 'g-', alpha=0.8, linewidth=2, \n",
    "             label=f'Trend: slope={z_res[0]:.6f}')\n",
    "    \n",
    "    ax2.set_title('Residuals vs Predicted')\n",
    "    ax2.set_xlabel('Predicted Count')\n",
    "    ax2.set_ylabel('Residuals')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"üìä Scatter plot analysis completed for {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb07ba2f",
   "metadata": {},
   "source": [
    "## 8. Error Analysis by Time Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d25254",
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_df is not None and 'timestamp' in test_df.columns:\n",
    "    # Add time components\n",
    "    test_df['hour'] = test_df['timestamp'].dt.hour\n",
    "    test_df['day_of_week'] = test_df['timestamp'].dt.dayofweek\n",
    "    test_df['month'] = test_df['timestamp'].dt.month\n",
    "    test_df['abs_error'] = np.abs(test_df['true_count'] - test_df['pred_count'])\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle(f'Error Analysis by Time Patterns: {MODEL_NAME.upper()}', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Error by hour of day\n",
    "    ax1 = axes[0, 0]\n",
    "    hourly_error = test_df.groupby('hour')['abs_error'].agg(['mean', 'std']).reset_index()\n",
    "    ax1.bar(hourly_error['hour'], hourly_error['mean'], alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    ax1.errorbar(hourly_error['hour'], hourly_error['mean'], yerr=hourly_error['std'], \n",
    "                 fmt='none', color='red', capsize=3)\n",
    "    ax1.set_title('Average Absolute Error by Hour of Day')\n",
    "    ax1.set_xlabel('Hour of Day')\n",
    "    ax1.set_ylabel('Mean Absolute Error')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_xticks(range(0, 24, 2))\n",
    "    \n",
    "    # 2. Error by day of week\n",
    "    ax2 = axes[0, 1]\n",
    "    daily_error = test_df.groupby('day_of_week')['abs_error'].agg(['mean', 'std']).reset_index()\n",
    "    days = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "    ax2.bar(range(7), daily_error['mean'], alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "    ax2.errorbar(range(7), daily_error['mean'], yerr=daily_error['std'], \n",
    "                 fmt='none', color='red', capsize=3)\n",
    "    ax2.set_title('Average Absolute Error by Day of Week')\n",
    "    ax2.set_xlabel('Day of Week')\n",
    "    ax2.set_ylabel('Mean Absolute Error')\n",
    "    ax2.set_xticks(range(7))\n",
    "    ax2.set_xticklabels(days)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Error by month\n",
    "    ax3 = axes[1, 0]\n",
    "    monthly_error = test_df.groupby('month')['abs_error'].agg(['mean', 'std']).reset_index()\n",
    "    ax3.bar(monthly_error['month'], monthly_error['mean'], alpha=0.7, color='coral', edgecolor='black')\n",
    "    ax3.errorbar(monthly_error['month'], monthly_error['mean'], yerr=monthly_error['std'], \n",
    "                 fmt='none', color='red', capsize=3)\n",
    "    ax3.set_title('Average Absolute Error by Month')\n",
    "    ax3.set_xlabel('Month')\n",
    "    ax3.set_ylabel('Mean Absolute Error')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.set_xticks(range(1, 13))\n",
    "    \n",
    "    # 4. Correlation heatmap\n",
    "    ax4 = axes[1, 1]\n",
    "    correlation_data = test_df[['true_count', 'pred_count', 'abs_error', 'hour', 'day_of_week', 'month']]\n",
    "    correlation_matrix = correlation_data.corr()\n",
    "    \n",
    "    im = ax4.imshow(correlation_matrix, cmap='coolwarm', aspect='auto', vmin=-1, vmax=1)\n",
    "    ax4.set_xticks(range(len(correlation_matrix.columns)))\n",
    "    ax4.set_yticks(range(len(correlation_matrix.columns)))\n",
    "    ax4.set_xticklabels(correlation_matrix.columns, rotation=45)\n",
    "    ax4.set_yticklabels(correlation_matrix.columns)\n",
    "    ax4.set_title('Correlation Matrix')\n",
    "    \n",
    "    # Add correlation values\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(len(correlation_matrix.columns)):\n",
    "            ax4.text(j, i, f'{correlation_matrix.iloc[i, j]:.2f}',\n",
    "                    ha=\"center\", va=\"center\", color=\"black\" if abs(correlation_matrix.iloc[i, j]) < 0.5 else \"white\")\n",
    "    \n",
    "    plt.colorbar(im, ax=ax4)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"üìä Time pattern analysis completed for {MODEL_NAME}\")\n",
    "    \n",
    "    # Print insights\n",
    "    print(f\"\\nüîç TIME PATTERN INSIGHTS:\")\n",
    "    worst_hour = hourly_error.loc[hourly_error['mean'].idxmax(), 'hour']\n",
    "    best_hour = hourly_error.loc[hourly_error['mean'].idxmin(), 'hour']\n",
    "    print(f\"  Worst performing hour: {worst_hour:02d}:00 (MAE: {hourly_error.loc[hourly_error['hour']==worst_hour, 'mean'].iloc[0]:.2f})\")\n",
    "    print(f\"  Best performing hour:  {best_hour:02d}:00 (MAE: {hourly_error.loc[hourly_error['hour']==best_hour, 'mean'].iloc[0]:.2f})\")\n",
    "    \n",
    "    worst_day = daily_error.loc[daily_error['mean'].idxmax(), 'day_of_week']\n",
    "    best_day = daily_error.loc[daily_error['mean'].idxmin(), 'day_of_week']\n",
    "    print(f\"  Worst performing day: {days[worst_day]} (MAE: {daily_error.loc[daily_error['day_of_week']==worst_day, 'mean'].iloc[0]:.2f})\")\n",
    "    print(f\"  Best performing day:  {days[best_day]} (MAE: {daily_error.loc[daily_error['day_of_week']==best_day, 'mean'].iloc[0]:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb07573b",
   "metadata": {},
   "source": [
    "## 9. Model Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cd5d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and compare all available models\n",
    "def compare_all_models():\n",
    "    \"\"\"Compare performance of all available models\"\"\"\n",
    "    comparison_results = []\n",
    "    \n",
    "    for model in available_models:\n",
    "        try:\n",
    "            test_df_model, _ = load_model_predictions(model)\n",
    "            if test_df_model is not None:\n",
    "                metrics = calculate_comprehensive_metrics(test_df_model, \"Test\")\n",
    "                metrics['Model'] = model\n",
    "                comparison_results.append(metrics)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Could not load {model}: {e}\")\n",
    "    \n",
    "    if comparison_results:\n",
    "        comparison_df = pd.DataFrame(comparison_results)\n",
    "        comparison_df = comparison_df[['Model', 'MAE', 'RMSE', 'MAPE (%)', 'R¬≤', 'Median AE', 'Max Error']]\n",
    "        comparison_df = comparison_df.sort_values('MAE')\n",
    "        \n",
    "        print(f\"\\nüèÜ MODEL COMPARISON SUMMARY\")\n",
    "        print(\"=\" * 80)\n",
    "        print(comparison_df.to_string(index=False, float_format='%.4f'))\n",
    "        \n",
    "        # Create comparison visualization\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        fig.suptitle('Model Comparison', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # MAE comparison\n",
    "        ax1 = axes[0]\n",
    "        bars1 = ax1.bar(comparison_df['Model'], comparison_df['MAE'], color='skyblue', edgecolor='black')\n",
    "        ax1.set_title('Mean Absolute Error (MAE)')\n",
    "        ax1.set_ylabel('MAE')\n",
    "        ax1.tick_params(axis='x', rotation=45)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar in bars1:\n",
    "            height = bar.get_height()\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                    f'{height:.2f}', ha='center', va='bottom')\n",
    "        \n",
    "        # R¬≤ comparison\n",
    "        ax2 = axes[1]\n",
    "        bars2 = ax2.bar(comparison_df['Model'], comparison_df['R¬≤'], color='lightgreen', edgecolor='black')\n",
    "        ax2.set_title('R-squared (R¬≤)')\n",
    "        ax2.set_ylabel('R¬≤')\n",
    "        ax2.tick_params(axis='x', rotation=45)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar in bars2:\n",
    "            height = bar.get_height()\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                    f'{height:.3f}', ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return comparison_df\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Run model comparison\n",
    "if len(available_models) > 1:\n",
    "    print(\"üîÑ Loading all models for comparison...\")\n",
    "    comparison_results = compare_all_models()\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è Only one model available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f28742",
   "metadata": {},
   "source": [
    "## 10. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d982b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary report\n",
    "if test_df is not None:\n",
    "    print(f\"\\nüìÑ ANALYSIS SUMMARY FOR {MODEL_NAME.upper()}\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"üìä Dataset: {len(test_df):,} test samples\")\n",
    "    if 'timestamp' in test_df.columns:\n",
    "        print(f\"üìÖ Time period: {test_df['timestamp'].min()} to {test_df['timestamp'].max()}\")\n",
    "    \n",
    "    print(f\"\\nüéØ Key Performance Metrics:\")\n",
    "    print(f\"  ‚Ä¢ MAE (Mean Absolute Error): {test_metrics['MAE']:.4f}\")\n",
    "    print(f\"  ‚Ä¢ RMSE (Root Mean Square Error): {test_metrics['RMSE']:.4f}\")\n",
    "    print(f\"  ‚Ä¢ MAPE (Mean Absolute Percentage Error): {test_metrics['MAPE (%)']:.2f}%\")\n",
    "    print(f\"  ‚Ä¢ R¬≤ (Coefficient of Determination): {test_metrics['R¬≤']:.4f}\")\n",
    "    \n",
    "    # Calculate additional insights\n",
    "    residuals = test_df['true_count'] - test_df['pred_count']\n",
    "    print(f\"\\nüìà Additional Insights:\")\n",
    "    print(f\"  ‚Ä¢ Mean residual (bias): {np.mean(residuals):.4f}\")\n",
    "    print(f\"  ‚Ä¢ Predictions within 10% of actual: {(np.abs(residuals/test_df['true_count']) <= 0.1).mean()*100:.1f}%\")\n",
    "    print(f\"  ‚Ä¢ Predictions within 20% of actual: {(np.abs(residuals/test_df['true_count']) <= 0.2).mean()*100:.1f}%\")\n",
    "    \n",
    "    # Export detailed results\n",
    "    export_df = test_df.copy()\n",
    "    export_df['residual'] = residuals\n",
    "    export_df['abs_error'] = np.abs(residuals)\n",
    "    export_df['pct_error'] = (residuals / test_df['true_count']) * 100\n",
    "    \n",
    "    export_filename = f\"{MODEL_NAME}_detailed_analysis.csv\"\n",
    "    export_df.to_csv(export_filename, index=False)\n",
    "    print(f\"\\nüíæ Detailed analysis exported to: {export_filename}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Analysis complete for {MODEL_NAME.upper()}!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
